"""
Script Ä‘á»ƒ cÃ i Ä‘áº·t vÃ  cáº¥u hÃ¬nh Ollama local LLM
"""
import subprocess
import sys
import os
import requests
import time

def run_command(command):
    """Cháº¡y command vÃ  tráº£ vá» káº¿t quáº£"""
    try:
        result = subprocess.run(
            command, 
            shell=True, 
            capture_output=True, 
            text=True,
            encoding='utf-8',
            errors='ignore'  # Ignore encoding errors
        )
        return result.returncode == 0, result.stdout, result.stderr
    except Exception as e:
        return False, "", str(e)

def download_ollama():
    """Download vÃ  cÃ i Ä‘áº·t Ollama"""
    print("ğŸ”„ Downloading Ollama...")
    
    if os.name == 'nt':  # Windows
        ollama_url = "https://ollama.com/download/windows"
        print(f"ğŸ“¥ Vui lÃ²ng táº£i Ollama tá»«: {ollama_url}")
        print("ğŸ’¡ Sau khi cÃ i Ä‘áº·t xong, cháº¡y láº¡i script nÃ y")
        return False
    else:  # Linux/Mac
        success, output, error = run_command("curl -fsSL https://ollama.com/install.sh | sh")
        if success:
            print("âœ… Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t thÃ nh cÃ´ng!")
            return True
        else:
            print(f"âŒ Lá»—i khi cÃ i Ä‘áº·t Ollama: {error}")
            return False

def check_ollama_installed():
    """Kiá»ƒm tra xem Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t chÆ°a"""
    success, output, error = run_command("ollama --version")
    return success

def start_ollama_service():
    """Khá»Ÿi Ä‘á»™ng Ollama service"""
    print("ğŸ”„ Khá»Ÿi Ä‘á»™ng Ollama service...")
    
    if os.name == 'nt':  # Windows
        success, output, error = run_command("ollama serve")
    else:  # Linux/Mac
        success, output, error = run_command("ollama serve &")
    
    # Äá»£i service khá»Ÿi Ä‘á»™ng
    time.sleep(5)
    return check_ollama_running()

def check_ollama_running():
    """Kiá»ƒm tra xem Ollama service cÃ³ Ä‘ang cháº¡y khÃ´ng"""
    try:
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def pull_model(model_name="llama3.2"):
    """Táº£i model tá»« Ollama"""
    print(f"ğŸ“¦ Äang táº£i model {model_name}...")
    print("âš ï¸  QuÃ¡ trÃ¬nh nÃ y cÃ³ thá»ƒ máº¥t vÃ i phÃºt...")
    
    try:
        # Use Popen for better control over long-running process
        process = subprocess.Popen(
            f"ollama pull {model_name}",
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.STDOUT,
            text=True,
            encoding='utf-8',
            errors='replace',
            bufsize=1,
            universal_newlines=True
        )
        
        # Show progress
        while True:
            output = process.stdout.readline()
            if output == '' and process.poll() is not None:
                break
            if output:
                # Filter out non-essential progress messages
                if any(keyword in output.lower() for keyword in ['pulling', 'verifying', 'writing', 'success']):
                    print(f"ğŸ“¥ {output.strip()}")
        
        return_code = process.poll()
        
        if return_code == 0:
            print(f"âœ… Model {model_name} Ä‘Ã£ Ä‘Æ°á»£c táº£i thÃ nh cÃ´ng!")
            return True
        else:
            print(f"âŒ Lá»—i khi táº£i model (return code: {return_code})")
            return False
            
    except Exception as e:
        print(f"âŒ Lá»—i khi táº£i model: {e}")
        return False

def list_available_models():
    """Liá»‡t kÃª cÃ¡c model cÃ³ sáºµn"""
    print("ğŸ“‹ CÃ¡c model Ä‘Æ°á»£c khuyáº¿n nghá»‹:")
    models = {
        "llama3.2": "Model tá»•ng quÃ¡t tá»‘t, nháº¹ (2GB)",
        "llama3.2:3b": "Model nhá», phÃ¹ há»£p mÃ¡y yáº¿u (2GB)", 
        "qwen2.5:7b": "Model máº¡nh cho coding (4.7GB)",
        "codellama:7b": "ChuyÃªn vá» coding (3.8GB)",
        "mistral:7b": "CÃ¢n báº±ng tá»‘t (4.1GB)"
    }
    
    for model, description in models.items():
        print(f"  â€¢ {model}: {description}")
    
    return models

def test_model(model_name="llama3.2"):
    """Test model vá»›i cÃ¢u há»i Ä‘Æ¡n giáº£n"""
    print(f"ğŸ§ª Testing model {model_name}...")
    
    test_prompt = "Hello! Please introduce yourself briefly."
    
    try:
        # Use Popen with proper encoding handling
        process = subprocess.Popen(
            f'ollama run {model_name} "{test_prompt}"',
            shell=True,
            stdout=subprocess.PIPE,
            stderr=subprocess.PIPE,
            text=True,
            encoding='utf-8',
            errors='replace'
        )
        
        stdout, stderr = process.communicate(timeout=30)
        
        if process.returncode == 0:
            print("âœ… Model hoáº¡t Ä‘á»™ng tá»‘t!")
            print("ğŸ“¤ Pháº£n há»“i tá»« AI:")
            print(stdout)
            return True
        else:
            print(f"âŒ Lá»—i khi test model: {stderr}")
            return False
            
    except subprocess.TimeoutExpired:
        process.kill()
        print("âŒ Test timeout - model cÃ³ thá»ƒ Ä‘ang load")
        return False
    except Exception as e:
        print(f"âŒ Lá»—i khi test model: {e}")
        return False

def main():
    print("ğŸ¤– Local AI Assistant Setup")
    print("=" * 40)
    
    # 1. Kiá»ƒm tra Ollama Ä‘Ã£ cÃ i Ä‘áº·t
    if not check_ollama_installed():
        print("âŒ Ollama chÆ°a Ä‘Æ°á»£c cÃ i Ä‘áº·t")
        download_ollama()
        return
    
    print("âœ… Ollama Ä‘Ã£ Ä‘Æ°á»£c cÃ i Ä‘áº·t")
    
    # 2. Kiá»ƒm tra service cÃ³ cháº¡y khÃ´ng
    if not check_ollama_running():
        print("ğŸ”„ Ollama service chÆ°a cháº¡y, Ä‘ang khá»Ÿi Ä‘á»™ng...")
        if not start_ollama_service():
            print("âŒ KhÃ´ng thá»ƒ khá»Ÿi Ä‘á»™ng Ollama service")
            print("ğŸ’¡ Thá»­ cháº¡y: ollama serve")
            return
    
    print("âœ… Ollama service Ä‘ang cháº¡y")
    
    # 3. Hiá»ƒn thá»‹ cÃ¡c model cÃ³ sáºµn
    models = list_available_models()
    
    # 4. Cho phÃ©p user chá»n model
    print("\nğŸ¯ Chá»n model Ä‘á»ƒ táº£i:")
    model_choice = input("Nháº­p tÃªn model (máº·c Ä‘á»‹nh: llama3.2): ").strip()
    if not model_choice:
        model_choice = "llama3.2"
    
    # 5. Táº£i model
    if pull_model(model_choice):
        # 6. Test model
        test_model(model_choice)
        
        print("\nğŸ‰ Setup hoÃ n táº¥t!")
        print(f"ğŸ’¬ Äá»ƒ chat vá»›i AI, sá»­ dá»¥ng: ollama run {model_choice}")
        print("ğŸ”§ Tiáº¿p theo, cháº¡y main AI assistant: python src/assistant/main.py")
    else:
        print("âŒ Setup tháº¥t báº¡i")

if __name__ == "__main__":
    main()